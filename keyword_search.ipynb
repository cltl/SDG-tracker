{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2-step keyword extraction\n",
    "#Text Mining Domains, VU\n",
    "\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "def new_dict_concat_six (unpickled_dict):\n",
    "    '''\n",
    "    takes an unpickled dictionary as input with headline as [1] and list of first five sentences as [2]\n",
    "    converts to a new dict with same key but value is string of concatenated headline + first five sentences\n",
    "    filters the values of the keywords\n",
    "    reurns: new dictionary with keys and concatenated headlines and first_five sentences of those articles which \n",
    "        contain the keywords\n",
    "    '''        \n",
    "    output = [unpickled_dict[key][1]+ '. ' + ' '.join(unpickled_dict[key][2]) for key in unpickled_dict]\n",
    "    \n",
    "    # calling test_dict only gets keys\n",
    "    new_dict = dict(zip(unpickled_dict, output))\n",
    "    \n",
    "    return new_dict\n",
    "\n",
    "def first_filter(keywords, infolder):\n",
    "    for file in glob.glob(infolder):\n",
    "        #prep filepaths for writing files\n",
    "        path = os.path.split(file)[0].split('/')[0]\n",
    "        basename = os.path.basename(file)\n",
    "\n",
    "        unpickled_dict = pickle.load(open (file, 'rb'))\n",
    "        #print(unpickled_dict)\n",
    "        # run the function above to concat headlines and first_five\n",
    "        new_dict = new_dict_concat_six(unpickled_dict)\n",
    "        #print(new_dict)\n",
    "        #rough filter for only keywords (see above)\n",
    "        output_dict = {k: v for k, v in new_dict.items() for keyword in keywords if keyword in v}\n",
    "\n",
    "        #write filtered articles - choosing json this time so I can inspect them in IDE\n",
    "        if not os.path.exists(f'filteredpickles/{keywords[0]}'):\n",
    "            os.makedirs(f'filteredpickles/{keywords[0]}')\n",
    "\n",
    "        with open(f'filteredpickles/{keywords[0]}/{basename.rstrip(\".gz.pkl\")}.json', 'w', encoding = 'utf-8') as outfile:\n",
    "            json.dump(output_dict, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rough filter, choose articles containing 'poverty' or 'aid',\n",
    "#write them to json files\n",
    "#computationally the most heavy step\n",
    "keywords=['poverty', ' aid']\n",
    "infolder = 'pickles/*'\n",
    "first_filter(keywords, infolder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase 2: fine-grained filter\n",
    "\n",
    "df_list = []\n",
    "for file in glob.glob('filteredpickles/*/*'):\n",
    "    jsonners = json.load(open(file, 'rb'))\n",
    "    columns=['text']\n",
    "    df_list.append(pd.DataFrame.from_dict(jsonners, orient='index', columns= columns))\n",
    "       \n",
    "big_df = pd.concat(df_list)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "195588\n"
     ]
    }
   ],
   "source": [
    "print(len(big_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_search(keywords, big_df, treshold = 0):\n",
    "    articles = []\n",
    "    article_keywords = []\n",
    "    #do a keyword search for the company\n",
    "    for text in big_df['text']:\n",
    "        keywords_found = set()\n",
    "        for keyword in keywords:\n",
    "            if keyword in text:\n",
    "                keywords_found.add(keyword)\n",
    "        if len(keywords_found) >= treshold:\n",
    "            articles.append(text)\n",
    "            article_keywords.append(keywords_found)\n",
    "    return articles, article_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#currently works for: poverty, gender equality (need to fix prefilter for the latter)\n",
    "topic = 'gender equality'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of articles found: 102\n"
     ]
    }
   ],
   "source": [
    "with open(f'keywords/{topic}.txt', 'r', encoding = 'utf-8') as infile:\n",
    "    keywords = infile.read().splitlines()\n",
    "\n",
    "#fine-grained search, kwarg treshold indicates the number of keywords that should be present in the text\n",
    "articles, keywords_per_article = keyword_search(keywords, big_df, treshold = 3)\n",
    "\n",
    "print('Number of articles found:', len(articles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(articles)):\n",
    "    print(keywords_per_article[i])\n",
    "    print(articles[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "def keyword_company_search(company, keywords, big_df, treshold = 0):\n",
    "    articles = []\n",
    "    article_keywords = []\n",
    "    \n",
    "    found_company = False\n",
    "    for text in big_df['text']:\n",
    "        if company in text:   \n",
    "            keywords_found = set()\n",
    "            for keyword in keywords:\n",
    "                if keyword in text:\n",
    "                    keywords_found.add(keyword)\n",
    "            if len(keywords_found) >= treshold:\n",
    "                articles.append(text)\n",
    "                article_keywords.append(keywords_found)\n",
    "    return articles, article_keywords\n",
    "\n",
    "with open('keywords/poverty.txt', 'r', encoding = 'utf-8') as infile:\n",
    "    keywordz = infile.read().splitlines()\n",
    "\n",
    "articles, keywords_per_article = keyword_company_search('Shell', keywordz, big_df, treshold = 3)\n",
    "\n",
    "print(len(articles))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
